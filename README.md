# Replicate LLM Model API | Voiceflow

This code utilizes Open AI GPT, Langchain, HNSWLib and Cheerio to fetch web content from URLs, create embeddings/vectors and save them in a local database. The knowledge base can then be used with GPT to answer questions.


## Node.js
If you are running this on Node.js 16, either:

run your application with NODE_OPTIONS='--experimental-fetch' node ..., or
install node-fetch and follow the instructions <a href="https://github.com/node-fetch/node-fetch#providing-global-access" target="_blank" rel="noopener noreferrer">here</a>

If you are running this on Node.js 18 or 19, you do not need to do anything.


## Setup
Setup is simple, just run:

```
npm i
npm start
```

This app requires an `.env` file with `PORT` and `REPLICATE_TOKEN`.
You can rename the .env.example file to .env and fill in the values.

## Endpoint

### `/api`

This endpoint allows a **POST** request with the **prompt**, the **model** name and the **settings** for this model.

**REQUEST**
```json
{
	"prompt":"Who was Dolly the sheep?",
	"model":"dolly-v2-12b",
	"max_tokens": 500,
  "top_p":1,
  "temperature": 0.75,
  "repetition_penalty": 1.2,
	"decoding":"top_p"
}
```

**RERSPONSE**
```json
{
	"success": true,
	"response": "Dolly could have been any sheep, but she especially became famous because she was the first successfully cloned mammal\n\n"
}
```

## Available models

Here is a list of the models you can use with this API.
Of course, you can update the `model.json` file as you want to add/remove models.

| Model Name | Description |
| --- | --- |
| `dolly-v2-12b` | Databricks | https://replicate.com/replicate/dolly-v2-12b |
| `stablelm-tuned-alpha-7b` | Stability AI | https://replicate.com/stability-ai/stablelm-tuned-alpha-7b |
| `flan-t5-xl` | Google | https://replicate.com/replicate/flan-t5-xl |
| `llama-7b` | Meta AI | https://replicate.com/replicate/llama-7b |
| `oasst-sft-1-pythia-12b` | Open-Assistant | https://replicate.com/replicate/oasst-sft-1-pythia-12b |

The `split` setting for each model can be set to true or false. It's used to join the response array into a string for model that returns an array of strings.


## Using ngrok

To allow access to the app externally using the port set in the `.env` file, you can use ngrok. Follow the steps below:

1. Install ngrok: https://ngrok.com/download
2. Run `ngrok http <port>` in your terminal (replace `<port>` with the port set in your `.env` file)
3. Copy the ngrok URL generated by the command and use it in your Voiceflow Assistant API step.

This can be handy if you want to quickly test this in an API step within your Voiceflow Assistant.
